{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "# model_name = \"distilbert/distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Language Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb5172b0d9d4a7db70fdac50e51ec2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load in 8bits is only possible with GPU (wrapper for cuda)\n",
    "# model = AutoModelForCausalLM.from_pretrained(mn, device_map=0, load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                model_name,\n",
    "                                # device_map=\"auto\",\n",
    "                                torch_dtype = torch_dtype,\n",
    "                                )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Tokenization\n",
    "We first have to encode our string into a set of tokens through an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Define the tokenizer\n",
    "tokr = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prompt used for the LLM\n",
    "prompt = \"Albert Einstein was a \"\n",
    "\n",
    "# Tokenizing the prompt and moving it to device\n",
    "toks = tokr([prompt], return_tensors='pt').to(device) # figure out what return tensors here is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 12560, 25721,   403,   264, 28705]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the string from our tokens we perform the inverse operation and decode the `input_ids` tensor from the above dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Albert Einstein was a ']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokr.batch_decode(toks['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/Users/fabianjaeger/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Albert Einstein was a '}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe = pipeline(task='text-generation', model=model, tokenizer=tokr)\n",
    "# pipe(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate response\n",
    "\n",
    "Autoregressively feed the input into the model to generate consecutive words\n",
    "\n",
    "Ensure that both the `toks` and the `model` are on the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 4.43 s, total: 46.7 s\n",
      "Wall time: 47.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 12560, 25721,   403,   264, 28705, 28750, 28734,   362, 28733,\n",
       "         15900,  5567, 28733,  6363,  3256,   294,   392,   693,  6202,   272,\n",
       "          5742,   302,  1016, 28283, 28725,   624]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "max_tok= 20\n",
    "\n",
    "# UNDERSTAND THE LOOP BELOW OF AUTOREGRESSIVE FEEDING INTO THE MODEL\n",
    "res = model.generate(**toks.to(device=device), max_new_tokens=max_tok).to('cpu')\n",
    "# res = model.generate(**toks, max_new_tokens=20)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> Albert Einstein was a 20th-century German-born physicist who developed the theory of relativity, one of the two pillars of modern physics. He']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokr.batch_decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(p, maxlen = 15, sample = True):\n",
    "    toks = tokr(p, return_tensors='pt')\n",
    "    res = model.generate(**toks.to(device), max_new_tokens = maxlen, do_sample = sample).to('cpu')\n",
    "    # res = model.generate(**toks.to(device), do_sample = sample).to('cpu') \n",
    "    return tokr.batch_decode(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(prompt, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stable Beluga (Instructional trained)\n",
    "\n",
    "For some models, the format for the prompts is found on the respective huggingface website. The prompt format for Stable Beluga 2 is\n",
    "\n",
    "```\n",
    "### System:\n",
    "This is a system prompt, please behave and help the user.\n",
    "\n",
    "### User:\n",
    "Your prompt here\n",
    "\n",
    "### Assistant:\n",
    "The output of Stable Beluga 2\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_sys = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can.\\n\\n\"\n",
    "def mk_prompt(user, syst=sb_sys):\n",
    "    return f\"{syst}###User: {user}\\n\\n### Assistant:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33045d8d1904c95bbd3943d484e94a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3102d92982949738d066c8c628470a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3a3e3115e94d4aa2c473addbdef714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abaf73cad3049058ed0b1a8546a75b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2d144070434dd5abd2abec7104c3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eda249075614849979a6d99bf657f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d43ae685ec4ecf8cf7031952977ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"stabilityai/StableBeluga-7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(mk_prompt(prompt), maxlen= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
